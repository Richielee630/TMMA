{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547c3bf9-1f60-4db8-a290-bca9e612c7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay\n",
    "overlay = Overlay(\"/home/ubuntu/workspace/pynq_bitfiles/2-24/MatMul_SA6.bit\")\n",
    "accel_ip = overlay.mmult_accel_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdcef6d1-6327-4acd-9c82-5b1d7a6c7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_fpga(A_buf, B_buf, C_buf, accel_ip, N, K, M, update_A):\n",
    "    \"\"\"\n",
    "    Runs a 2D matrix multiplication on the FPGA accelerator:\n",
    "      (N, K) x (K, M) => (N, M)\n",
    "\n",
    "    A_buf, B_buf, C_buf are PYNQ buffers allocated with shape=(N,K), (K,M), (N,M).\n",
    "    update_A: 1 to load A into BRAM (new input), 0 to reuse persistent A.\n",
    "    \"\"\"\n",
    "    print(\"calling fpga, update_A =\", update_A)\n",
    "    \n",
    "    # Flush input buffers to ensure data consistency.\n",
    "    # Only flush A_buf if we intend to update A (update_A==1).\n",
    "    if update_A:\n",
    "        A_buf.flush()\n",
    "    B_buf.flush()\n",
    "\n",
    "    # Configure the accelerator registers\n",
    "    accel_ip.register_map.A_1 = A_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.A_2 = (A_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.B_1 = B_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.B_2 = (B_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.C_1 = C_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.C_2 = (C_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.N = N\n",
    "    accel_ip.register_map.K = K\n",
    "    accel_ip.register_map.M = M\n",
    "    # Pass the update_A flag to the accelerator\n",
    "    accel_ip.register_map.update_A = update_A\n",
    "\n",
    "    # Start the accelerator\n",
    "    accel_ip.register_map.CTRL.AP_START = 1\n",
    "\n",
    "    # Wait for finish\n",
    "    while accel_ip.register_map.CTRL.AP_DONE == 0:\n",
    "        pass\n",
    "\n",
    "    # Invalidate output buffer so the CPU sees the updated data from DDR\n",
    "    C_buf.invalidate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9c9a-d51f-45a2-9574-827ca1d3e0b5",
   "metadata": {},
   "source": [
    "# Block 1: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25665a2f-2e09-45fc-8d92-ad87ceea071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pynq import allocate\n",
    "\n",
    "def pynq_buffer_from_numpy(np_array):\n",
    "    \"\"\"\n",
    "    Allocates a PYNQ buffer with the same shape and dtype as np_array,\n",
    "    then copies the data into the buffer.\n",
    "    \"\"\"\n",
    "    buf = allocate(np_array.shape, dtype=np_array.dtype)\n",
    "    np.copyto(buf, np_array)\n",
    "    return buf\n",
    "\n",
    "def requantize(int32_array, scale, zero_point=0):\n",
    "    \"\"\"\n",
    "    Requantizes an int32 numpy array to int8 using the provided scale and zero_point.\n",
    "    Operation: int8_val = clip(round(int32_val * scale + zero_point), -128, 127)\n",
    "    \"\"\"\n",
    "    scaled = np.round(int32_array * scale + zero_point)\n",
    "    int8_array = np.clip(scaled, -128, 127).astype(np.int8)\n",
    "    return int8_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268799-d711-4d89-a410-b7ebecd98f43",
   "metadata": {},
   "source": [
    "# Block 2: Custom Module for FPGA Offload\n",
    "#### This module wraps a quantized linear layer (for Q, K, or V) and in its forward pass it quantizes its input (if needed), converts the activation and weight to PYNQ buffers, calls the FPGA accelerator viaâ€¯call_fpga(), and then dequantizes the resulting int32 accumulation back to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ab79b8-7967-45bf-be6f-8d05b2115576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class FPGAQuantizedLinear(torch.nn.Module):\n",
    "    def __init__(self, quantized_linear, act_scale, accel_ip, hidden_size=768, update_A=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          quantized_linear : an instance of DynamicQuantizedLinear from the quantized model.\n",
    "          act_scale        : scaling factor for quantizing input activations.\n",
    "          accel_ip         : the FPGA accelerator IP handle.\n",
    "          hidden_size      : hidden dimension size (typically 768).\n",
    "          update_A         : flag indicating whether to update A in persistent BRAM (True for Q, False for K/V).\n",
    "        \"\"\"\n",
    "        super(FPGAQuantizedLinear, self).__init__()\n",
    "        self.accel_ip = accel_ip\n",
    "        self.hidden_size = hidden_size\n",
    "        self.act_scale = act_scale\n",
    "        self.update_A = update_A  # Store the update flag\n",
    "        \n",
    "        # Extract quantized weight and its parameters.\n",
    "        self.weight_int8_tensor = quantized_linear.weight().int_repr()\n",
    "        self.weight_scale = quantized_linear.weight().q_scale()\n",
    "        self.weight_zero_point = quantized_linear.weight().q_zero_point()\n",
    "        # Transpose so that the weight shape becomes (in_features, out_features)\n",
    "        self.weight_int8 = self.weight_int8_tensor.cpu().numpy().T  # shape: (hidden_size, hidden_size)\n",
    "        \n",
    "        # Effective scale: multiplication of activation scale and weight scale.\n",
    "        self.effective_scale = self.act_scale * self.weight_scale\n",
    "        \n",
    "        # Check for bias. Note that in DynamicQuantizedLinear, bias remains in FP32.\n",
    "        bias_val = quantized_linear.bias()  # This calls the bound method.\n",
    "        if bias_val is not None:\n",
    "            # Save bias as a NumPy array (shape: (hidden_size,))\n",
    "            self.bias = bias_val.detach().cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for FPGA offload.\n",
    "        Accepts input x which may be 2D (N, D) or 3D (B, S, D). In case of 3D input,\n",
    "        the tensor is reshaped to 2D for matrix multiplication and then reshaped back.\n",
    "        The input is quantized to int8 using self.act_scale. After the FPGA multiplication,\n",
    "        the int32 result is dequantized to FP32 and the bias is added (if available).\n",
    "        \"\"\"\n",
    "        # Save the original shape.\n",
    "        orig_shape = x.shape\n",
    "        if x.dim() == 3:\n",
    "            B, S, D = x.shape\n",
    "            x_flat = x.reshape(B * S, D)\n",
    "        else:\n",
    "            x_flat = x\n",
    "\n",
    "        # Determine the number of rows for the FPGA call.\n",
    "        N = x_flat.shape[0]\n",
    "\n",
    "        # Quantize the input if it is in float32.\n",
    "        if x_flat.dtype == torch.float32:\n",
    "            x_int8 = torch.clamp(torch.round(x_flat / self.act_scale), -128, 127).to(torch.int8)\n",
    "        else:\n",
    "            x_int8 = x_flat\n",
    "\n",
    "        # Convert to a NumPy int8 array.\n",
    "        x_np = x_int8.cpu().numpy().astype(np.int8)\n",
    "        \n",
    "        # Convert input activation and weight to PYNQ buffers.\n",
    "        A_buf = pynq_buffer_from_numpy(x_np)\n",
    "        W_buf = pynq_buffer_from_numpy(self.weight_int8)\n",
    "        # Allocate an output buffer for the int32 result (shape: (N, hidden_size))\n",
    "        C_buf = allocate((N, self.hidden_size), dtype=np.int32)\n",
    "        \n",
    "        # Call the FPGA accelerator:\n",
    "        # Instead of hardcoding update_A=1, we now use self.update_A:\n",
    "        call_fpga(A_buf, W_buf, C_buf, self.accel_ip, N, self.hidden_size, self.hidden_size, update_A=int(self.update_A))\n",
    "        \n",
    "        # Retrieve the int32 result.\n",
    "        C_int32 = np.array(C_buf)\n",
    "        # Dequantize: convert int32 accumulator to FP32 using the effective scale.\n",
    "        out_fp32 = C_int32.astype(np.float32) * self.effective_scale\n",
    "        \n",
    "        # If a bias is present, add it (broadcast along axis 0).\n",
    "        if self.bias is not None:\n",
    "            # Ensure bias is added to each row.\n",
    "            out_fp32 = out_fp32 + self.bias\n",
    "        \n",
    "        # Convert back to a torch tensor.\n",
    "        out_tensor = torch.tensor(out_fp32, dtype=torch.float32)\n",
    "        \n",
    "        # If the original input was 3D, reshape back to (B, S, hidden_size).\n",
    "        if x.dim() == 3:\n",
    "            out_tensor = out_tensor.reshape(B, S, self.hidden_size)\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb0937-70a5-48f9-b85f-05abe5f02d19",
   "metadata": {},
   "source": [
    "# Block 3: Function to Replace Q, K, V Layers with FPGA-Offloaded Versions\n",
    "#### This function walks through all transformer layers in the quantized DistilBERT model and replaces the Q, K, and V projection layers with our custom FPGA-accelerated module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eece722d-3257-4872-9f48-600b16cc2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_fpga_offload(model_quant, act_scale, accel_ip, hidden_size=768):\n",
    "    \"\"\"\n",
    "    Replaces the Q, K, V projection layers in each transformer layer with the FPGA-accelerated custom module.\n",
    "    \n",
    "    Parameters:\n",
    "      model_quant  : Quantized DistilBertForSequenceClassification model.\n",
    "      act_scale    : Scaling factor for quantizing activations (assumed same for demo).\n",
    "      accel_ip     : Configured FPGA accelerator IP handle.\n",
    "      hidden_size  : Hidden dimension (typically 768).\n",
    "    \"\"\"\n",
    "    for layer in model_quant.distilbert.transformer.layer:\n",
    "        # For the Q projection, set update_A to True so that the persistent A is updated.\n",
    "        layer.attention.q_lin = FPGAQuantizedLinear(layer.attention.q_lin, act_scale, accel_ip, hidden_size, update_A=True)\n",
    "        # For K and V projections, set update_A to False to reuse A from BRAM.\n",
    "        layer.attention.k_lin = FPGAQuantizedLinear(layer.attention.k_lin, act_scale, accel_ip, hidden_size, update_A=False)\n",
    "        layer.attention.v_lin = FPGAQuantizedLinear(layer.attention.v_lin, act_scale, accel_ip, hidden_size, update_A=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc067c9-f8eb-4b46-8aec-fbc5df161aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_activation_scale(activation_list, percentile=99.9, use_demo=0):\n",
    "    \"\"\"\n",
    "    Computes a global activation scale from a calibration set of activations.\n",
    "    \n",
    "    Parameters:\n",
    "      activation_list: List of NumPy arrays representing activations \n",
    "                       (for example, from the embedding layer).\n",
    "      percentile:      The percentile to use for robust scale computation (if use_demo=0).\n",
    "      use_demo:        If set to 1, uses the demo method (scale = max_abs_value/127.0);\n",
    "                       otherwise, uses the robust method (scale = percentile_value/127.0).\n",
    "    \n",
    "    Returns:\n",
    "      A scaling factor computed as:\n",
    "         - Demo method: scale = (max(|activations|)) / 127.0\n",
    "         - Robust method: scale = (percentile(|activations|)) / 127.0\n",
    "    \"\"\"\n",
    "    # Concatenate all activations from the calibration samples into one array.\n",
    "    all_activations = np.concatenate([act.flatten() for act in activation_list])\n",
    "    \n",
    "    if use_demo:\n",
    "        # Demo method: use the maximum absolute value.\n",
    "        act_abs_max = np.max(np.abs(all_activations))\n",
    "        scale = act_abs_max / 127.0 if act_abs_max != 0 else 1.0\n",
    "    else:\n",
    "        # Robust method: use the specified percentile.\n",
    "        act_abs_percentile = np.percentile(np.abs(all_activations), percentile)\n",
    "        scale = act_abs_percentile / 127.0 if act_abs_percentile != 0 else 1.0\n",
    "    \n",
    "    return scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e9e58-20e7-4c5f-ba65-63b500df50f1",
   "metadata": {},
   "source": [
    "# Block 4: Example Usage â€“ Custom Forward Pass Integration\n",
    "#### This block shows how to load and quantize the model, extract an activation from the embedding layer, integrate the FPGA offload into the modelâ€™s transformer layers, and run a forward pass. (For demonstration, only the Q, K, V projections are offloaded; the rest of the model remains unchanged.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515ef581-29af-467a-9375-83eaf32e3fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Activation Scale (Demo): 0.06596306177574819\n",
      "input = 'The moonlight shimmered over the ocean as waves gently kissed the sandy shore, while distant lanterns flickered in the cool evening breeze. A lone traveler wandered along the coastline, footsteps sinking softly into the damp sand, lost in thought. The rhythmic sound of the water mixed with the rustling palms, creating a nice'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Assume call_fpga() is already defined and accel_ip is configured on your KV260.\n",
    "# For example:\n",
    "# accel_ip = get_accel_ip_handle()   # <-- user-specific setup\n",
    "\n",
    "# 1. Load and Quantize the Model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Apply dynamic quantization to convert Linear layers to int8.\n",
    "model_int8 = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "model_int8.eval()\n",
    "\n",
    "# 2. Gather a Calibration Set of Activations to Compute a Global Activation Scale\n",
    "calib_sentences = [\n",
    "    \"The moonlight shimmered over the ocean as waves gently kissed the sandy shore, while distant lanterns flickered in the cool evening breeze. A lone traveler wandered along the coastline, footsteps sinking softly into the damp sand, lost in thought. The rhythmic sound of the water mixed with the rustling palms, creating a nice\"\n",
    "]\n",
    "calib_activations = []\n",
    "for sentence in calib_sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        # Get the embedding output; shape: (B, L, 768). Here B=1.\n",
    "        emb = model.distilbert.embeddings(inputs.input_ids)  # shape: (1, L, 768)\n",
    "        # Remove the batch dimension.\n",
    "        emb = emb.squeeze(0)  # shape: (L, 768)\n",
    "        calib_activations.append(emb.cpu().numpy())\n",
    "\n",
    "# Compute the activation scale using the robust method (percentile-based):\n",
    "# global_act_scale_robust = compute_activation_scale(calib_activations, percentile=99.9, use_demo=0)\n",
    "# print(\"Global Activation Scale (Robust):\", global_act_scale_robust)\n",
    "\n",
    "# # Compute the activation scale using the demo method (max-based):\n",
    "global_act_scale_demo = compute_activation_scale(calib_activations, use_demo=1)\n",
    "print(\"Global Activation Scale (Demo):\", global_act_scale_demo)\n",
    "\n",
    "test_sentence = calib_sentences[0]\n",
    "print(f\"input = '{test_sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901297f5-4db5-472e-a3bb-5d250af638c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Inference Time: 0.553830 seconds\n",
      "CPU Logits: tensor([[-3.1795,  3.4633]])\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 1\n",
      "calling fpga, update_A = 0\n",
      "calling fpga, update_A = 0\n",
      "FPGA Offloaded Inference Time: 1.228901 seconds\n",
      "FPGA Logits: tensor([[-2.9969,  3.2308]])\n",
      "Speedup (CPU vs. FPGA): 0.45x\n",
      "Max Logits Difference: 0.232455\n",
      "Mean Logits Difference: 0.207528\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# CPU-only Inference\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\")\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs_cpu = model_int8(inputs.input_ids)\n",
    "    logits_cpu = outputs_cpu.logits\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "print(f\"CPU Inference Time: {cpu_time:.6f} seconds\")\n",
    "print(\"CPU Logits:\", logits_cpu)\n",
    "\n",
    "# FPGA-Offloaded Inference\n",
    "integrate_fpga_offload(model_int8, global_act_scale_demo, accel_ip, hidden_size=768)\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs_fpga = model_int8(inputs.input_ids)\n",
    "    logits_fpga = outputs_fpga.logits\n",
    "fpga_time = time.time() - start_time\n",
    "\n",
    "print(f\"FPGA Offloaded Inference Time: {fpga_time:.6f} seconds\")\n",
    "print(\"FPGA Logits:\", logits_fpga)\n",
    "\n",
    "# Compare Speedup\n",
    "speedup = cpu_time / fpga_time\n",
    "print(f\"Speedup (CPU vs. FPGA): {speedup:.2f}x\")\n",
    "\n",
    "# Compare Logits Difference\n",
    "logits_diff = torch.abs(logits_cpu - logits_fpga)\n",
    "max_diff = torch.max(logits_diff).item()\n",
    "mean_diff = torch.mean(logits_diff).item()\n",
    "\n",
    "print(f\"Max Logits Difference: {max_diff:.6f}\")\n",
    "print(f\"Mean Logits Difference: {mean_diff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacf98f-f068-41c0-b3c6-a0d199d5e45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
