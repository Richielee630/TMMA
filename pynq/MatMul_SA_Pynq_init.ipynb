{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa07b91-8733-462a-b608-603f7160e4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay\n",
    "overlay = Overlay(\"/home/ubuntu/workspace/pynq_bitfiles/2-19/MatMul_SA5.bit\")\n",
    "overlay.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355884be-fe84-4477-97b2-c617bcf11075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current IP PL clock hz: 99.999Mhz\n"
     ]
    }
   ],
   "source": [
    "from pynq import Clocks\n",
    "print(f\"current IP PL clock hz: {Clocks.fclk0_mhz}Mhz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069e1e8a-b548-4173-889d-6b3f7620fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mmult_accel_0', 'zynq_ultra_ps_e_0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlay.ip_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2cbcc6-79de-46e3-94ad-c5e70464499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'phys_addr': 0xB0000000, 'addr_range': 0x00010000\n"
     ]
    }
   ],
   "source": [
    "ip_info = overlay.ip_dict['mmult_accel_0']\n",
    "\n",
    "# Extract relevant fields\n",
    "phys_addr = ip_info['phys_addr']\n",
    "addr_range = ip_info['addr_range']\n",
    "\n",
    "# Print in 8-bit hex format\n",
    "print(f\"'phys_addr': 0x{phys_addr:08X}, 'addr_range': 0x{addr_range:08X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d9341b-4088-4203-b1d4-9ef7d15a9b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0xfeedbeef\n"
     ]
    }
   ],
   "source": [
    "accel_ip = overlay.mmult_accel_0  # or whatever the block name is\n",
    "# Write to an offset (e.g., 0x10) if thatâ€™s your control register\n",
    "accel_ip.write(0x10, 0xFEEDBEEF)\n",
    "# Read back\n",
    "val = accel_ip.read(0x10)\n",
    "print(hex(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d03268-89f5-4a76-9b49-33cc84607394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterMap {\n",
      "  CTRL = Register(AP_START=0, AP_DONE=0, AP_IDLE=1, AP_READY=0, RESERVED_1=0, AUTO_RESTART=0, RESERVED_2=0, INTERRUPT=0, RESERVED_3=0),\n",
      "  GIER = Register(Enable=0, RESERVED=0),\n",
      "  IP_IER = Register(CHAN0_INT_EN=0, CHAN1_INT_EN=0, RESERVED_0=0),\n",
      "  IP_ISR = Register(CHAN0_INT_ST=0, CHAN1_INT_ST=0, RESERVED_0=0),\n",
      "  A_1 = Register(A=write-only),\n",
      "  A_2 = Register(A=write-only),\n",
      "  B_1 = Register(B=write-only),\n",
      "  B_2 = Register(B=write-only),\n",
      "  C_1 = Register(C=write-only),\n",
      "  C_2 = Register(C=write-only),\n",
      "  N = Register(N=write-only),\n",
      "  K = Register(K=write-only),\n",
      "  M = Register(M=write-only)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(accel_ip.register_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1486f5ac-7407-49b4-9cd7-f8530020234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pynq import MMIO\n",
    "\n",
    "# # phys_addr = 0xA0000000, addr_range = 0x10000\n",
    "# ip_base_addr = 2684354560   # æˆ–è€…ç›´æŽ¥å†™ 0xA0000000\n",
    "# ip_addr_range = 65536       # æˆ–è€… 0x10000\n",
    "\n",
    "# mmult_mmio = MMIO(ip_base_addr, ip_addr_range)\n",
    "\n",
    "# # çŽ°åœ¨å°±å¯ä»¥å¯¹å¯„å­˜å™¨è¿›è¡Œè¯»å†™äº†ï¼Œä¾‹å¦‚ï¼š\n",
    "# mmult_mmio.write(0x10, 1234)         # å¾€ offset=0x10 å†™å…¥ 1234\n",
    "# val = mmult_mmio.read(0x10)          # è¯»å›ž offset=0x10\n",
    "# print(\"Read value:\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bb570a-d55d-48cd-9b4b-c4452bd09739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_fpga(A_buf, B_buf, C_buf, accel_ip, N, K, M):\n",
    "    \"\"\"\n",
    "    Runs a 2D matrix multiplication on the FPGA accelerator:\n",
    "      (N, K) x (K, M) => (N, M)\n",
    "\n",
    "    A_buf, B_buf, C_buf are PYNQ buffers allocated with shape=(N,K), (K,M), (N,M).\n",
    "    \"\"\"\n",
    "    print(\"calling fpga\")\n",
    "    \n",
    "    # 1. Flush input buffers so data is written to memory\n",
    "    A_buf.flush()\n",
    "    B_buf.flush()\n",
    "\n",
    "    # 2. Configure the accelerator\n",
    "    accel_ip.register_map.A_1 = A_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.A_2 = (A_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.B_1 = B_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.B_2 = (B_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.C_1 = C_buf.physical_address & 0xFFFFFFFF\n",
    "    accel_ip.register_map.C_2 = (C_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "    accel_ip.register_map.N = N\n",
    "    accel_ip.register_map.K = K\n",
    "    accel_ip.register_map.M = M\n",
    "\n",
    "    # 3. Start the accelerator\n",
    "    accel_ip.register_map.CTRL.AP_START = 1\n",
    "\n",
    "    # 4. Wait for finish\n",
    "    while accel_ip.register_map.CTRL.AP_DONE == 0:\n",
    "        pass\n",
    "\n",
    "    # 5. Invalidate output buffer (so CPU sees fresh data)\n",
    "    C_buf.invalidate()\n",
    "\n",
    "    # Return no explicit result here since C_buf is updated in place\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8216ea-dff5-479f-be5a-f817b53b9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pynq import allocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6d1e9e-eba8-409f-88c2-13f7687a2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Prepare Buffers (Using a small example for demonstration)\n",
    "# Suppose we want to multiply a [N, K] x [K, M] => [N, M]\n",
    "# DistilBERT typical Q/K/V matmul: shape [batch*seq_len, 768] x [768, 768]\n",
    "# For demonstration, letâ€™s do a small random size. Weâ€™ll adapt logic for DistilBERT below.\n",
    "N, K, M = 64, 768, 768\n",
    "A_buf = allocate(shape=(N, K), dtype=np.int8, cacheable=False)\n",
    "B_buf = allocate(shape=(K, M), dtype=np.int8, cacheable=False)\n",
    "C_buf = allocate(shape=(N, M), dtype=np.int32, cacheable=False) # accelerator writes int32 results\n",
    "\n",
    "# 3) Initialize Input Data (simulate random int8 values)\n",
    "A_buf[:] = np.random.randint(-128, 127, size=(N,K), dtype=np.int8)\n",
    "B_buf[:] = np.random.randint(-128, 127, size=(K,M), dtype=np.int8)\n",
    "# A_buf[:] = np.zeros((N, K), dtype=np.int8)\n",
    "# B_buf[:] = np.zeros((K, M), dtype=np.int8)\n",
    "\n",
    "# flush caches so data is in DDR\n",
    "A_buf.flush()\n",
    "B_buf.flush()\n",
    "\n",
    "# 4) Configure Accelerator Control Registers\n",
    "# Set Input & Output Buffer Addresses to Accelerator\n",
    "accel_ip.register_map.A_1 = A_buf.physical_address & 0xFFFFFFFF\n",
    "accel_ip.register_map.A_2 = (A_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "accel_ip.register_map.B_1 = B_buf.physical_address & 0xFFFFFFFF\n",
    "accel_ip.register_map.B_2 = (B_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "accel_ip.register_map.C_1 = C_buf.physical_address & 0xFFFFFFFF\n",
    "accel_ip.register_map.C_2 = (C_buf.physical_address >> 32) & 0xFFFFFFFF\n",
    "accel_ip.register_map.N = N\n",
    "accel_ip.register_map.K = K\n",
    "accel_ip.register_map.M = M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd1e66b-fca9-4b1e-ba6b-6cfe50e88535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A_buf phys_addr': 0x375F0000\n",
      "'B_bufphys_addr': 0x38000000\n",
      "IP Base Address:  0xb0000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"'A_buf phys_addr': 0x{A_buf.physical_address:08X}\")\n",
    "print(f\"'B_bufphys_addr': 0x{B_buf.physical_address:08X}\")\n",
    "print(\"IP Base Address: \", hex(accel_ip.mmio.base_addr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53896b53-b25b-4ab2-ac3a-c35c3a8ad03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling fpga\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ============================================\n",
    "# ðŸš€ Step 3: BENCHMARK - FPGA ACCELERATOR\n",
    "# ============================================\n",
    "\n",
    "# FPGA Execution\n",
    "start_data_in = time.time()\n",
    "A_buf.flush()\n",
    "B_buf.flush()\n",
    "end_data_in = time.time()\n",
    "\n",
    "start_fpga = time.time()\n",
    "# accel_ip.register_map.CTRL.AP_START = 1\n",
    "# while accel_ip.register_map.CTRL.AP_DONE == 0:\n",
    "#     pass\n",
    "call_fpga(A_buf, B_buf, C_buf, accel_ip, N, K, M)\n",
    "end_fpga = time.time()\n",
    "\n",
    "start_data_out = time.time()\n",
    "C_buf.invalidate()\n",
    "result_fpga = C_buf[:, :].copy()\n",
    "end_data_out = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f735f1a9-d2d3-4af5-a663-a3ac4128e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ðŸ’» Step 4: CPU REFERENCE (NumPy & PyTorch)\n",
    "# ===========================\n",
    "start_cpu_numpy = time.time()\n",
    "A_sw = A_buf.astype(np.int32)\n",
    "B_sw = B_buf.astype(np.int32)\n",
    "ref_numpy = A_sw @ B_sw\n",
    "# ref_numpy = -1\n",
    "end_cpu_numpy = time.time()\n",
    "\n",
    "start_cpu_torch = time.time()\n",
    "device = torch.device(\"cpu\")\n",
    "A_torch = torch.tensor(A_buf.astype(np.int32), device=device)\n",
    "B_torch = torch.tensor(B_buf.astype(np.int32), device=device)\n",
    "ref_torch = torch.matmul(A_torch, B_torch)\n",
    "ref_torch_np = ref_torch.cpu().numpy()\n",
    "end_cpu_torch = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49980d5c-a260-4cb6-98e8-b82cc1e98502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ§ª Step 5: ACCURACY CHECK\n",
    "# ============================\n",
    "diff_numpy = np.abs(ref_numpy - result_fpga)\n",
    "max_err_numpy = np.max(diff_numpy)\n",
    "\n",
    "diff_torch = np.abs(ref_torch_np - result_fpga)\n",
    "max_err_torch = np.max(diff_torch)\n",
    "\n",
    "# ============================\n",
    "# ðŸ“Š Step 6: PERFORMANCE METRICS\n",
    "# ============================\n",
    "total_ops = 2 * N * K * M\n",
    "\n",
    "acc_latency = end_fpga - start_fpga\n",
    "total_hw_time = end_data_out - start_data_in\n",
    "hw_throughput = (total_ops / acc_latency) / 1e9\n",
    "hw_end_to_end = (total_ops / total_hw_time) / 1e9\n",
    "\n",
    "sw_time_numpy = end_cpu_numpy - start_cpu_numpy\n",
    "sw_throughput_numpy = (total_ops / sw_time_numpy) / 1e9\n",
    "speedup_latency_numpy = sw_time_numpy / acc_latency\n",
    "speedup_total_numpy = sw_time_numpy / total_hw_time\n",
    "\n",
    "sw_time_torch = end_cpu_torch - start_cpu_torch\n",
    "sw_throughput_torch = (total_ops / sw_time_torch) / 1e9\n",
    "speedup_latency_torch = sw_time_torch / acc_latency\n",
    "speedup_total_torch = sw_time_torch / total_hw_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "356f1560-3fd7-44cc-9c6a-512d25d1f280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-145470   33986  256613 ... -257507  267542   41925]\n",
      " [  72113  319012   59307 ...   30549  -32042 -164951]\n",
      " [ 267984   73130  -54719 ...  -10391 -119638  196044]\n",
      " ...\n",
      " [-275360   53641  -80537 ...   49469    6966  -48150]\n",
      " [  93598 -200453   18880 ...  -49456 -135556  252617]\n",
      " [  16469 -174141  -89092 ...  241539   17946   54187]]\n",
      "[[-145470   33986  256613 ... -257507  267542   41925]\n",
      " [  72113  319012   59307 ...   30549  -32042 -164951]\n",
      " [ 267984   73130  -54719 ...  -10391 -119638  196044]\n",
      " ...\n",
      " [-275360   53641  -80537 ...   49469    6966  -48150]\n",
      " [  93598 -200453   18880 ...  -49456 -135556  252617]\n",
      " [  16469 -174141  -89092 ...  241539   17946   54187]]\n"
     ]
    }
   ],
   "source": [
    "print(result_fpga)\n",
    "print(ref_torch_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed243760-be79-40b2-b3e8-f0830fba8b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Performance Comparison\n",
      "ðŸ”¢ MatMul Shape: [64 , 768] x [768 , 768]\n",
      "âœ… Max difference (NumPy vs FPGA): 0\n",
      "âœ… Max difference (PyTorch vs FPGA): 0\n",
      "\n",
      "ðŸ“Š Latency & Throughput\n",
      "ðŸ§® NumPy  : â±ï¸ 1.229659 sec |  âš¡ 0.06 GFLOPs\n",
      "ðŸ”¥ PyTorch: â±ï¸ 0.098479 sec  |  âš¡ 0.77 GFLOPs\n",
      "ðŸš€ FPGA   : â±ï¸ 0.039595 sec  |  âš¡ 1.91 GFLOPs\n",
      "\n",
      "â±ï¸ Total HW Time : â±ï¸ 0.044839 sec  |  âš¡ 1.68 GFLOPs\n",
      "\n",
      "ðŸš€ FPGA Speedup\n",
      "ðŸš€ðŸš€ðŸš€ Speedup vs NumPy   : 31.06x (Latency) | 27.42x (Total)\n",
      "ðŸš€ Speedup vs PyTorch : 2.49x (Latency) | 2.20x (Total)\n",
      "\n",
      "âœ… Test Completed! ðŸŽ¯\n"
     ]
    }
   ],
   "source": [
    "# Determine emoji for accuracy check\n",
    "diff_numpy_emoji = \"âŒ\" if max_err_numpy != 0 else \"âœ…\"\n",
    "diff_torch_emoji = \"âŒ\" if max_err_torch != 0 else \"âœ…\"\n",
    "\n",
    "# Define speedup emoji function\n",
    "def speedup_emoji(speedup):\n",
    "    if speedup >= 25:\n",
    "        return \"ðŸš€ðŸš€ðŸš€\"\n",
    "    elif speedup >= 10:\n",
    "        return \"ðŸš€ðŸš€\"\n",
    "    elif speedup >= 1:\n",
    "        return \"ðŸš€\"\n",
    "    return \"ðŸ¢\"\n",
    "\n",
    "# ================================\n",
    "# ðŸŽ¯ Step 7: PERFORMANCE SUMMARY\n",
    "# ================================\n",
    "print(\"\\nðŸŽ¯ Performance Comparison\")\n",
    "print(f\"ðŸ”¢ MatMul Shape: [{N} , {K}] x [{K} , {M}]\")\n",
    "\n",
    "print(f\"{diff_numpy_emoji} Max difference (NumPy vs FPGA): {max_err_numpy}\")\n",
    "print(f\"{diff_torch_emoji} Max difference (PyTorch vs FPGA): {max_err_torch}\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Latency & Throughput\")\n",
    "print(f\"ðŸ§® NumPy  : â±ï¸ {sw_time_numpy:.6f} sec |  âš¡ {sw_throughput_numpy:.2f} GFLOPs\")\n",
    "print(f\"ðŸ”¥ PyTorch: â±ï¸ {sw_time_torch:.6f} sec  |  âš¡ {sw_throughput_torch:.2f} GFLOPs\")\n",
    "print(f\"ðŸš€ FPGA   : â±ï¸ {acc_latency:.6f} sec  |  âš¡ {hw_throughput:.2f} GFLOPs\\n\")\n",
    "\n",
    "print(f\"â±ï¸ Total HW Time : â±ï¸ {total_hw_time:.6f} sec  |  âš¡ {hw_end_to_end:.2f} GFLOPs\\n\")\n",
    "\n",
    "print(\"ðŸš€ FPGA Speedup\")\n",
    "print(f\"{speedup_emoji(speedup_latency_numpy)} Speedup vs NumPy   : {speedup_latency_numpy:.2f}x (Latency) | {speedup_total_numpy:.2f}x (Total)\")\n",
    "print(f\"{speedup_emoji(speedup_latency_torch)} Speedup vs PyTorch : {speedup_latency_torch:.2f}x (Latency) | {speedup_total_torch:.2f}x (Total)\")\n",
    "\n",
    "print(\"\\nâœ… Test Completed! ðŸŽ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea8a7a-7b8f-4672-bfb9-f7eb42669b4d",
   "metadata": {},
   "source": [
    "### @TODO: CPU + FPGA hybrid LLM Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6dbb8-0ab2-4bf6-bfa6-f30b0b7bbcb9",
   "metadata": {},
   "source": [
    "\n",
    "## Example: Integrating with DistilBERT (PyTorch) for Q, K, V, Out Projection in MHA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eca18fa6-af65-4512-9fbc-c55fc0ad49dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "# Load the DistilBERT base-uncased model\n",
    "distilbert_model = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model.eval() # set inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf88eb05-08d0-419a-8612-9c0af81eeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose we have an input tokenized and loaded into a torch tensor\n",
    "# tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# text = \"The moonlight shimmered over the quiet ocean as waves gently kissed the sandy shore, while distant lanterns flickered in the cool evening breeze. A lone traveler wandered along the coastline, footsteps sinking softly into the damp sand, lost in thought. The rhythmic sound of the water mixed with the rustling palms, creating a nice\"\n",
    "# tokens = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6fcbb271-288b-4166-b272-d5be5b0404ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# # torch.set_num_threads(1)  # Set to 1 thread to avoid CPU parallelism\n",
    "# # torch.backends.mkldnn.enabled = False  # Disable MKL-DNN optimizations\n",
    "# # torch.backends.openmp.enabled = False  # Disable OpenMP (multi-threading for CPU)\n",
    "\n",
    "# start_db_cpu = time.time()\n",
    "# with torch.no_grad():\n",
    "#     # we can run the model once fully on CPU, just to see the normal output\n",
    "#     cpu_output = distilbert_model(**tokens)\n",
    "# end_db_cpu = time.time()\n",
    "# print(\"CPU DistilBERT last hidden state shape:\", cpu_output.last_hidden_state.shape)\n",
    "# print(f\"cpu run time: {(end_db_cpu-start_db_cpu):.6f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35c052de-4b7b-4c5e-bd71-845dddef1e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynq import allocate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def distilbert_fpga_qkv_projection(\n",
    "    hidden_states,\n",
    "    q_weight, k_weight, v_weight,\n",
    "    q_bias, k_bias, v_bias,\n",
    "    accel_ip\n",
    "):\n",
    "    \"\"\"\n",
    "    Offloads Q, K, V projection in a single pass each\n",
    "    for an entire sequence (batch=1).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_dim = hidden_states.shape  # e.g., [1, 64, 768]\n",
    "    # Weight shapes: [hidden_dim, qkv_dim], e.g. [768, 768] if head_dim * num_heads = 768\n",
    "    _, qkv_dim = q_weight.shape\n",
    "\n",
    "    # We assume batch_size=1 for simplicity\n",
    "    assert batch_size == 1, \"This example handles only batch_size=1. Loop over batch otherwise.\"\n",
    "\n",
    "    # Shape for A is (64, 768), B is (768, qkv_dim), C is (64, qkv_dim)\n",
    "    N = seq_len\n",
    "    K = hidden_dim\n",
    "    M = qkv_dim\n",
    "\n",
    "    # Convert Torch -> NumPy\n",
    "    A_np = hidden_states[0].cpu().numpy().astype(np.int8)  # shape (64, 768)\n",
    "    Qw_np = q_weight.cpu().numpy().astype(np.int8)        # shape (768, qkv_dim)\n",
    "    Kw_np = k_weight.cpu().numpy().astype(np.int8)\n",
    "    Vw_np = v_weight.cpu().numpy().astype(np.int8)\n",
    "\n",
    "    # Allocate PYNQ buffers\n",
    "    A_buf  = allocate(shape=(N, K), dtype=np.int8)\n",
    "    Qw_buf = allocate(shape=(K, M), dtype=np.int8)\n",
    "    Kw_buf = allocate(shape=(K, M), dtype=np.int8)\n",
    "    Vw_buf = allocate(shape=(K, M), dtype=np.int8)\n",
    "    CQ_buf = allocate(shape=(N, M), dtype=np.int32)  # output for Q\n",
    "    CK_buf = allocate(shape=(N, M), dtype=np.int32)  # output for K\n",
    "    CV_buf = allocate(shape=(N, M), dtype=np.int32)  # output for V\n",
    "\n",
    "    # Copy data to buffers\n",
    "    A_buf[:]  = A_np\n",
    "    Qw_buf[:] = Qw_np\n",
    "    Kw_buf[:] = Kw_np\n",
    "    Vw_buf[:] = Vw_np\n",
    "\n",
    "    # 1) Q = hidden_states Ã— q_weight\n",
    "    call_fpga(A_buf, Qw_buf, CQ_buf, accel_ip, N, K, M)\n",
    "    # 2) K = hidden_states Ã— k_weight\n",
    "    call_fpga(A_buf, Kw_buf, CK_buf, accel_ip, N, K, M)\n",
    "    # 3) V = hidden_states Ã— v_weight\n",
    "    call_fpga(A_buf, Vw_buf, CV_buf, accel_ip, N, K, M)\n",
    "\n",
    "    # Convert results back to NumPy float\n",
    "    Q_out_np = CQ_buf.copy().astype(np.float32)  # shape (64, qkv_dim)\n",
    "    K_out_np = CK_buf.copy().astype(np.float32)\n",
    "    V_out_np = CV_buf.copy().astype(np.float32)\n",
    "\n",
    "    # Free PYNQ buffers (optional, but good practice)\n",
    "    A_buf.freebuffer()\n",
    "    Qw_buf.freebuffer()\n",
    "    Kw_buf.freebuffer()\n",
    "    Vw_buf.freebuffer()\n",
    "    CQ_buf.freebuffer()\n",
    "    CK_buf.freebuffer()\n",
    "    CV_buf.freebuffer()\n",
    "\n",
    "    # Add bias & convert to Torch\n",
    "    # biases [qkv_dim], so shape (768,) or similar\n",
    "    Q_out = torch.tensor(Q_out_np, device=hidden_states.device) + q_bias\n",
    "    K_out = torch.tensor(K_out_np, device=hidden_states.device) + k_bias\n",
    "    V_out = torch.tensor(V_out_np, device=hidden_states.device) + v_bias\n",
    "\n",
    "    # Reshape back to (batch_size, seq_len, qkv_dim)\n",
    "    Q_out = Q_out.unsqueeze(0)  # shape [1, 64, qkv_dim]\n",
    "    K_out = K_out.unsqueeze(0)\n",
    "    V_out = V_out.unsqueeze(0)\n",
    "\n",
    "    return Q_out, K_out, V_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1722beed-24af-424f-958b-378acd04a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerBlock_FPGA(nn.Module):\n",
    "    def __init__(self, original_layer, accel_ip):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.accel_ip = accel_ip\n",
    "\n",
    "        # Extract submodules from the DistilBERT layer\n",
    "        self.attention = original_layer.attention\n",
    "        self.sa_layer_norm = original_layer.sa_layer_norm\n",
    "        self.ffn = original_layer.ffn\n",
    "        self.output_layer_norm = original_layer.output_layer_norm\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # 1) Grab Q, K, V weights and biases\n",
    "        q_weight = self.attention.q_lin.weight\n",
    "        k_weight = self.attention.k_lin.weight\n",
    "        v_weight = self.attention.v_lin.weight\n",
    "        q_bias   = self.attention.q_lin.bias\n",
    "        k_bias   = self.attention.k_lin.bias\n",
    "        v_bias   = self.attention.v_lin.bias\n",
    "\n",
    "        # 2) Offload Q, K, V in one call each\n",
    "        Q, K, V = distilbert_fpga_qkv_projection(\n",
    "            hidden_states,\n",
    "            q_weight, k_weight, v_weight,\n",
    "            q_bias, k_bias, v_bias,\n",
    "            self.accel_ip\n",
    "        )\n",
    "\n",
    "        # 3) Forward through attention\n",
    "        attn_output_tuple = self.attention(Q, K, V, mask=attention_mask)\n",
    "        attn_output = attn_output_tuple[0] if isinstance(attn_output_tuple, tuple) else attn_output_tuple\n",
    "\n",
    "        # 4) Residual connection & layer norm\n",
    "        hidden_states = self.sa_layer_norm(attn_output + hidden_states)\n",
    "\n",
    "        # 5) Feed-forward network\n",
    "        ffn_output = self.ffn(hidden_states)\n",
    "        hidden_states = self.output_layer_norm(ffn_output + hidden_states)\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11aa50dc-e0e5-4f7f-8dc2-882455f20203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBert_FPGA(nn.Module):\n",
    "    def __init__(self, original_model, accel_ip):\n",
    "        super().__init__()\n",
    "        self.embeddings = original_model.embeddings  # Keep embeddings unchanged\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock_FPGA(layer, accel_ip) for layer in original_model.transformer.layer\n",
    "        ])\n",
    "        self.model = original_model  # Keep reference for methods like `forward`\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.embeddings(input_ids)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "\n",
    "        return hidden_states  # No classifier, returning last hidden states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04d2ec2b-0c2f-49ce-82a9-226349f54974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "calling fpga\n",
      "CPU Result Shape:   torch.Size([1, 63, 768])\n",
      "FPGA Result Shape:  torch.Size([1, 63, 768])\n",
      "Mean Absolute Difference: 0.294033\n",
      "Max Absolute Difference:  9.816427\n",
      "Outputs differ by more than 1e-3. Check your FPGA calculations or quantization settings.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# 1) Load or define your normal CPU-based DistilBERT (same weights as distilbert_fpga)\n",
    "#    If you used DistilBertModel: \n",
    "original_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "#    If you used DistilBertForSequenceClassification:\n",
    "# original_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "#\n",
    "# Assume you've already done:\n",
    "distilbert_fpga = DistilBert_FPGA(original_model, accel_ip)\n",
    "\n",
    "# 2) Create tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 3) Sample input text\n",
    "text = (\n",
    "    \"The moonlight shimmered over the ocean as waves gently kissed the sandy shore, while distant lanterns flickered in the cool evening breeze. A lone traveler wandered along the coastline, footsteps sinking softly into the damp sand, lost in thought. The rhythmic sound of the water mixed with the rustling palms, creating a nice\"\n",
    ")\n",
    "\n",
    "# 4) Tokenize (CPU)\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]  # optional, if returned\n",
    "\n",
    "# 5) Run normal CPU inference\n",
    "# Make sure original_model is on CPU\n",
    "original_model.to(\"cpu\")\n",
    "original_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Output can be either 'last hidden states' or 'logits', depending on your model\n",
    "    # DistilBertModel returns a BaseModelOutput, we can access last_hidden_state\n",
    "    outputs_cpu = original_model(input_ids, attention_mask=attention_mask)\n",
    "    if isinstance(outputs_cpu, tuple):\n",
    "        # e.g., DistilBertModel returns (last_hidden_state, )\n",
    "        cpu_result = outputs_cpu[0]\n",
    "    else:\n",
    "        # e.g., DistilBertForSequenceClassification returns (logits, )\n",
    "        cpu_result = outputs_cpu.logits if hasattr(outputs_cpu, \"logits\") else outputs_cpu\n",
    "\n",
    "# 6) Run FPGA inference\n",
    "# distilbert_fpga is our custom model\n",
    "distilbert_fpga.eval()  # ensure evaluation mode\n",
    "with torch.no_grad():\n",
    "    fpga_result = distilbert_fpga(input_ids, attention_mask=attention_mask)\n",
    "    # If it returns a tuple, handle accordingly\n",
    "    if isinstance(fpga_result, tuple):\n",
    "        fpga_result = fpga_result[0]\n",
    "    elif hasattr(fpga_result, \"logits\"):\n",
    "        fpga_result = fpga_result.logits\n",
    "\n",
    "# 7) Compare results\n",
    "# We'll compute some difference metrics, like mean absolute difference (MAD)\n",
    "diff = (cpu_result.last_hidden_state - fpga_result).abs()\n",
    "mean_abs_diff = diff.mean().item()\n",
    "max_abs_diff = diff.max().item()\n",
    "\n",
    "print(\"CPU Result Shape:  \", cpu_result.last_hidden_state.shape)\n",
    "print(\"FPGA Result Shape: \", fpga_result.shape)\n",
    "print(f\"Mean Absolute Difference: {mean_abs_diff:.6f}\")\n",
    "print(f\"Max Absolute Difference:  {max_abs_diff:.6f}\")\n",
    "\n",
    "# For a quick pass/fail check:\n",
    "if mean_abs_diff < 1e-3:\n",
    "    print(\"The CPU and FPGA outputs match closely!\")\n",
    "else:\n",
    "    print(\"Outputs differ by more than 1e-3. Check your FPGA calculations or quantization settings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fdc55aee-5e07-4464-a1e0-45d40b37ace1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0687, -0.0838, -0.1465,  ...,  0.1789,  0.3295,  0.2308],\n",
       "         [ 0.1156,  0.2173, -0.5855,  ...,  0.5756,  0.6235, -0.0118],\n",
       "         [ 0.2028,  0.2943,  0.0740,  ...,  0.0933,  0.1457,  0.0222],\n",
       "         ...,\n",
       "         [-0.3366, -0.1055, -0.0118,  ..., -0.1256,  0.0316,  0.6437],\n",
       "         [-0.3558, -0.5757,  0.1040,  ...,  0.2259,  0.0016, -0.1222],\n",
       "         [-0.0130,  0.0660, -0.1378,  ...,  0.4973,  0.1388, -0.4929]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_result.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0aff4f01-6e85-4a8a-8190-6d1f517ef26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0458,  0.0643,  0.1332,  ...,  0.0055,  0.1163,  0.1951],\n",
       "         [-0.3400, -0.0190, -0.4053,  ...,  0.4973,  0.5385, -0.6715],\n",
       "         [ 0.1272,  0.1808,  0.2804,  ..., -0.3034,  0.2214, -0.2878],\n",
       "         ...,\n",
       "         [-0.6369,  0.3454,  0.1677,  ..., -0.1749,  0.0853,  0.3318],\n",
       "         [ 0.2345,  0.2810,  0.3364,  ..., -0.1767, -0.2867,  0.0592],\n",
       "         [-0.0959,  0.0636,  0.1333,  ...,  0.6422,  0.0584,  0.2774]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpga_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017d04b-76f7-4f56-b090-652bfde42215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
